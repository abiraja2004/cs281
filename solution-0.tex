% No 'submit' option for the problems by themselves.
%\documentclass{harvardml}
% Use the 'submit' option when you submit your solutions.
\documentclass[submit]{harvardml}

% Put in your full name and email address.
\name{Luis Antonio Perez}
\email{luisperez@college.harvard.edu}

% List any people you worked with.
\collaborators{%
}

% You don't need to change these.
\course{CS281-F15}
\assignment{Assignment \#0}
\duedate{11:59pm September 11, 2013}

\usepackage{url, enumitem}
\usepackage{amsfonts, amsmath}
\usepackage{listings}

% Some useful macros.
\newcommand{\given}{\,|\,}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\text{var}}
\newcommand{\cov}{\text{cov}}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent Please turn in your solutions \textbf{via canvas} by the due date listed above.
\\

\noindent This assignment is intended to ensure that you have the background required for CS281. You should be able to answer the problems below without complicated calculations. All questions are worth $70/6 = 11.\bar{6}$ points unless stated otherwise.

\begin{problem}[Variance and covariance]
Let $X$ and~$Y$ be two independent random variables.

\begin{enumerate}[label=(\alph*)]
\item Show that the independence of~$X$ and~$Y$ implies that their
covariance is zero.

\item For a scalar constant~$a$, show the following two properties:
\begin{align*}
  \E(X + aY) &= \E(X) + a\E(Y)\\
  \var(X + aY) &= \var(X) + a^2\var(Y)
\end{align*}
\end{enumerate}
\end{problem}

\begin{enumerate}[label=(\alph*)]
\item We first proof the fact that $\E(XY) = \E(X)\E(Y)$ if $X,Y$ are independent random variables. We make the assumption that $X,Y$ are continuous.
\begin{proof}
Let $f_X(x)$ and $f_Y(y)$ be the PDFs for $X,Y$, respectively, and $f_{X,Y}(x,y)$ the joint PDF. Then using the definition of expectation:
\begin{align*}
\E(XY) &= \int \int xyf_{X,Y}(x,y)dxdy \\
&= \int \int xyf_{X}(x)f_Y(y)dx dy \tag{independence of $X,Y$} \\
&= \left[\int xf_X(x)dx\right] \left[\int yf_Y(y)dy \right] \tag{properties of integrals}\\
&=\E(X)\E(Y)
\end{align*}
\end{proof}
A similar proof can be used to show the same fact for discrete variables. Given the above result, it is straighforward to show (a).
\begin{align*}
\cov(X,Y) &= \E(XY) - \E(X)E(Y) \\
&= \E(X)\E(Y) - \E(X)\E(Y) = 0 \tag{by independence of $X,Y$}
\end{align*}
(b) We now let $a \in \mathbb{R}$. Then we have:
\begin{align*}
\E(X + aY) &= \int \int (x+ay)f_{X,Y}(x,y) dx dy \\
&= \int \int xf_{X,Y}(x,y) dx dy + a\int \int y f_{X,Y}(x,y) dx dy \tag{linearity of integration} \\
&= \int \int xf_X(x)f_{Y|X}(y) dx dy + a \int \int yf_{Y}(y)f_{X|Y}(x)dx \tag{definition of joint distribution}\\
&= \left[\int xf_X(x) dx \right] \left[\int f_{Y|X}(y)dy \right] + a\left[\int yf_Y(y)dy \right]\left[\int f_{X|Y}(x)dx \right] \\
&= \left[\int xf_X(x) dx \right] + a\left[\int yf_Y(y)dy \right] \tag{integral on entire support is $1$} \\
&= \E(X) + a\E(Y)
\end{align*}
Now that we've shown linearity of expectation, we have the following:
\begin{align*}
\var(X + aY) &= \E[(X + aY)^2] - [\E(X + aY)]^2 \\
&= \E(X^2 + 2aXY + a^2Y^2) - [\E(X + aY)]^2 \\
&= \E(X^2) + 2a\E(XY) + a^2\E(Y^2) - (\E(X) + a\E(Y))^2 \tag{linearity of expectation}\\
&= \E(X^2) - \E(X)^2 + a^2(\E(Y^2) - \E(Y)^2) + 2a\E(XY) - 2a\E(X)\E(Y) \\
&= \var(X) + a^2\var(Y) \tag{$X,Y$ are independent}
\end{align*}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}[Densities]
Answer the following questions.
\begin{enumerate}[label=(\alph*)]
  \item Can a probability density function (pdf) ever take values greater than 1?
  \item Let $X$ be a univariate normally distributed random variable with mean 0 and variance $1/100$. What is the pdf of $X$?
  \item What is the value of this pdf at 0?
  \item What is the probability that $X = 0$?
  \item Explain the discrepancy.
\end{enumerate}
\end{problem}

\begin{enumerate}[label=(\alph*)]
\item Yes. The only restriction is that the integral over the support be $1$. See below for an example of such a PDF.
\item $f_X(x) = \frac{10}{\sqrt{2\pi}}\exp\{-50x^2\}$
\item $\frac{10}{\sqrt{2\pi}}$
\item The probability of the event $X = 0$ is almost surely $0$.
\item The pdf for a continous random variable does not measure the probability of the variable realizing a particular values $x$. Rather, it is the density of the probability distribution, and to obtain the actual probability one must integrate over an interval (more generally, a set of non-zero measure) within the support, typically $(x - \epsilon, x + \epsilon)$ for some $\epsilon > 0$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}[Conditioning and Bayes' rule]
Let $\mu \in \R^m$ and $\Sigma, \Sigma' \in \R^{m \times m}$. Let $X$ be an $m$-dimensional random vector with $X \sim \mathcal{N}(\mu, \Sigma)$, and let $Y$ be a $m$-dimensional random vector such that $Y \given X \sim \mathcal{N}(X, \Sigma')$. State how each of the following is distributed. (For example: ``normally distributed with mean $z$ and variance $s$.")
\begin{enumerate}[label=(\alph*)]
  \item The unconditional distribution of $Y$.
  \item The joint distribution of the random variable $(X,Y)$.
  \item The conditional distribution of $X$ given $Y$.
\end{enumerate}
\end{problem}

\begin{enumerate}[label=(\alph*)]
  \item The unconditional distribution of $Y$ is normally distributed with mean $\mu$ and variance $\Sigma + \Sigma'$. We can see this with the following. Consider the random variable $Y - X \mid X \sim \mathcal{N}(0,\Sigma')$. So $Y-X \mid X$ is normally distributed with mean $0$ and covariance matrix $\Sigma'$. This distribution does not depend on $X$, which implies that $Y - X \sim \mathcal{N}(0,\Sigma')$ (this is the unconditional distribution).

  Then $Y$ is simply the sum of $Z \sim \mathcal{N}(0,\Sigma')$ and $X \sim \mathcal{N}(\mu, \Sigma)$. Therefore, $Y \sim \mathcal{N}(\mu, \Sigma + \Sigma')$.
  \item For this, we follow closely the results presented in Murphy's Theorem 4.3.1. We have already calculated the two marginal distribution of $X,Y$ (both MVN). Then the joint distribution, according to Theorem 4.3.1, is also a multivariate normal with (we have MVN marginal and MVN conditional):
  \begin{align*}
    \mu' = (\mu \circ \mu) \in \mathbb{R}^{2m} \\
    \Sigma'' =  \left( \begin{array}{cc}
\Sigma & \Sigma\\
\Sigma & \Sigma + \Sigma' \end{array} \right) \in \mathbb{R}^{2m \times 2m}
  \end{align*}
  Note that $\circ : \mathbb{R}^m \times \mathbb{R}^m \to \mathbb{R}^{2m}$ is simply a function which concatenates two vectors.
We case see the above directly by noting:
\begin{align*}
\E((X \circ Y)) &= (\E(X) \circ \E(Y)) = (\mu \circ \mu) \\
\Sigma'' &= \left( \begin{array}{cc}
\var(X) & \cov(X,Y) \\
\cov(Y,X) & \var(Y) \end{array} \right)
\var(X) &= \Sigma \\
\var(Y) &= \Sigma + \Sigma' \\
\cov(X,Y) &= \E(\cov(X,Y \mid Z)) + \cov(\E(X|Z),\E(Y|Z)) \\
&= \E(\cov(X,Y \mid X)) + \cov(\E(X \mid X),\E(Y \mid X))\tag{let $Z = X$} \\
&= \cov(X,X) \\
&= \Sigma \\
\cov(Y,X) &= \E(\cov(Y,X \mid Z)) + \cov(\E(Y|Z), \E(X|Z)) \\
&= \cov(X,X) \\
&= \Sigma \\
\end{align*}
  \item Given the above results, and following the statements of the result in Murphy 4.4.1, the conditional distribution of $X \mid Y$ is normally distributed with mean $(\Sigma^{-1} + \Sigma'^{-1})^{-1}(\Sigma^{-1}\mu + \Sigma'^{-1}Y)$ and variance $(\Sigma^{-1} + \Sigma'^{-1})^{-1}$.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}[I can Ei-gen]
Let $X \in \R^{n \times m}$.
\begin{enumerate}[label=(\alph*)]
\item What is the relationship between the $n$ eigenvalues of $XX^T$ and the $m$ eigenvalues of $X^TX$?
\item Suppose $X$ is square (i.e., $n=m$) and symmetric. What does this tell you about the eigenvalues of $X$? What are the eigenvalues of $X + I$, where $I$ is the identity matrix?
\item Suppose $X$ is square, symmetric, and invertible. What are the eigenvalues of $X^{-1}$?
\end{enumerate}
\end{problem}

\begin{enumerate}[label=(\alph*)]
\item  Let $v$ be an eigenvector of $XX^T$ with eigenvalue $\lambda$. Then $X^Tv$ is an eigenvector of $X^TX$ with the same eigenvalue.
\begin{align*}
X^TX(X^Tv) &= X^T(XX^T)v \\
&= X^T(\lambda v) \\
&= \lambda(X^Tv)
\end{align*}
\item If we have a square, symmetric matrix, we know that all of the eigenvalues are real. The eigenvalues of $X+I$ are given by $\lambda_i + 1$ where $\lambda_i$ is an eigenvalue of $X$. This is because $(X+I)v = Xv + Iv = \lambda v + v = (\lambda + 1)v$.
\item If $\lambda_i$ is an eigenvalue of $X$, then $\frac{1}{\lambda_i}$ is an eigenvalue of $X^{-1}$. This is simple to verify since $XX^{-1}v = v \implies X^{-1}v = \frac{1}{\lambda}v$ because $\frac{1}{\lambda}Xv = \frac{1}{\lambda}{\lambda}v = v$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}[Calculus]
Let $x, y \in \R^m$ and $A \in \R^{m \times m}$. Please answer the following questions, writing your answers in vector notation.
\begin{enumerate}[label=(\alph*)]
\item What is the gradient with respect to $x$ of $x^T y$?
\item What is the gradient with respect to $x$ of $x^T x$?
\item What is the gradient with respect to $x$ of $x^T A x$?
\item What is the gradient with respect to $x$ of $A x$?
\end{enumerate}
\end{problem}

\begin{enumerate}[label=(\alph*)]
\item $y$.
\item $2x$.
\item $(A + A^T)x$.
\item $A$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}[Sanity check. This problem is worth 0 points.]
Bugs in machine learning software, as in any software, can be detected by running
tests to see if the code produces the required output for a particular input.
When one of these tests fails it is necessary to fix the code.  How do you usually track down where
the bug occurs?
\begin{itemize}
\item[A)] I use the debugger to figure out why things are behaving the way they are.
\item[B)] I add print statements to see the value of the different variables.
Scrolling through the resulting output is easier and more efficient than using the debugger.
\item[C)] If you are good at programming your code should not have bugs.
\end{itemize}
I use the debugger. It's nice to inspect the environment around where I believe the error has occurred. PDB is my go-to for Python, gdb for C,C++, and whatever the internet recommends for other languages.
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{problem}[Stability]
One way to solve numerical problems in the evaluation of a function is to compute
a power series approximation around the input that causes the problems.
Around what input is the following python function not numerically stable?
What is the problem?

\begin{lstlisting}[language=python]
import numpy as np
def buggy(x):
    return np.sin(x**2) / x
\end{lstlisting}
Use the power series approximation to write a numerically stable linear approximation of the function around the the problematic input. (Hint: recall L'H\^opital's rule.) Your answer should be of the following form.

\begin{lstlisting}[language=python]
import numpy as np
def linear_approximation_around_problematic_input(x):
    return ?? # return some expression here
\end{lstlisting}
\end{problem}

The function is not numerically stable around the point $x = 0$. The problem is that, mathematically, around $x = 0$ we are attempting to divide by a very small value and approaching the indeterminate form $\frac{0}{0}$.

In order to fix this, we can approximate the function around $x =0$ using a linear approximation. The easiest way to do this is to take the power series expansion of the function, and truncate all terms of with order $> 1$. For small $x$, this is valid.

Doing the above, we have:
\begin{align*}
\sin(x^2) &= \sum_{k=0}^{\infty} (-1)^{k}\frac{x^{2(2k+1)}}{(2k+1)!} \\
&\approx x^2 \tag{truncate $k > 0$ terms}
\end{align*}
With the above approximation, we have:
\begin{align*}
\frac{\sin (x^2)}{x} \approx \frac{x^2}{x} = x
\end{align*}
Therefore, we can approximate using the simple linear function:
\begin{lstlisting}[language=python]
import numpy as np
def linear_approximation_around_problematic_input(x):
    return x
\end{lstlisting}
\end{document}
