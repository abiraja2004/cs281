{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import autograd.numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from autograd.numpy.random import multivariate_normal as N\n",
    "from autograd.scipy.stats import norm as N1\n",
    "from autograd import grad\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "# Each row is formatted as [User ID] [Joke ID] [Rating]\n",
    "ratingsFile =  \"ratings.dat\"\n",
    "allData = pd.read_csv(ratingsFile, sep=\" \", header=None)\n",
    "allData = allData.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allData.columns = ['UserID', 'JokeID', 'Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numTrain = 100000\n",
    "numTest = 100000\n",
    "train = allData[:numTrain]\n",
    "test = allData[numTrain:numTrain + numTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainUserIDs = sorted(train['UserID'].unique())\n",
    "trainJokeIDs = sorted(train['JokeID'].unique())\n",
    "testUserIDs = sorted(test['UserID'].unique())\n",
    "testJokeIDs = sorted(test['JokeID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note that not all users in the testing set are in the training set! \n",
    "sorted(trainJokeIDs) == sorted(testJokeIDs), sorted(trainUserIDs) == sorted(testUserIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We precompute the ratings each user gave to the movies he or she rated\n",
    "def createMappings():\n",
    "    '''\n",
    "    Given a data set, creates the mappings necessary for quick access of results\n",
    "    '''\n",
    "    userIds = trainUserIDs\n",
    "    jokeIds = trainJokeIDs\n",
    "    \n",
    "    # This is a mapping from userID to an numpy array of ratings for that user.\n",
    "    userToRatings = {}\n",
    "    # This is a mapping from userID to a list of indexes corresponding to the jokes rated by the user.\n",
    "    userToJokeIDs = {}\n",
    "    \n",
    "    # Same as above, but in reverse.\n",
    "    jokeToRatings = {}\n",
    "    jokeToUserIDs = {}\n",
    "    \n",
    "    # Fill the mappings per users\n",
    "    for userID in userIds:\n",
    "        userToRatings[userID] = np.array(train[train.UserID == userID].Rating.values)\n",
    "        userToJokeIDs[userID] = np.array(train[train.UserID == userID].JokeID.values)\n",
    "        \n",
    "    for jokeID in jokeIds:\n",
    "        jokeToRatings[jokeID] = np.array(train[train.JokeID == jokeID].Rating.values)\n",
    "        jokeToUserIDs[jokeID] = np.array(train[train.JokeID == jokeID].UserID.values)\n",
    "        \n",
    "    return userToRatings, userToJokeIDs, jokeToRatings, jokeToUserIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainUtoR, trainUToJ, trainJtoR, trainJToU = createMappings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We're going to create a mapping from userID to row_index in U\n",
    "# Also, a mapping from jokeID to row_index in V\n",
    "# Additionally, a mapping from userID to row_index in V of jokes rated by the user\n",
    "# and a mapping from jokeID to row_index in U of the users that rated that joke\n",
    "def createInitialParams(K, sigma2u, sigma2v, users, jokes, UtoJ, JtoU):\n",
    "    '''\n",
    "    Returns initial parameters of our U,V distributions along with the mappings described in the table above.\n",
    "    Input parameters K, the number of laten variables, the variance of U,V distributions respectively,\n",
    "    a list of userIds, list of jokeIds, a mapping from userIDs to jokes they've rated, and a mapping from \n",
    "    jokeIDs to users that have rated them, in that order. \n",
    "    '''\n",
    "    # Maps from Users/Jokes to rows\n",
    "    UserToU = { user : i for i, user in enumerate(users)}\n",
    "    JokeToV = { joke : i for i, joke in enumerate(jokes)}\n",
    "    \n",
    "    # Now we create the more complicated mapping from Users to JokeRowsRater and Joke to UserRowsRatedBy\n",
    "    UserToV = { user: np.array([JokeToV[joke] for joke in UtoJ[user]]) for user in users}\n",
    "    JokeToU = { joke: np.array([UserToU[user] for user in JtoU[joke]]) for joke in jokes}\n",
    "    \n",
    "    # Lastly, we create our parameters for the normal distributions from which we sample U,V\n",
    "    mus = { user: N(np.ones(K), np.identity(K)) for user in users}\n",
    "    nus = { joke: N(np.ones(K), np.identity(K)) for joke in jokes}\n",
    "    Sigmas = { user: sigma2u * np.identity(K) for user in users}\n",
    "    Taus = { joke: sigma2v * np.identity(K) for joke in jokes}\n",
    "    \n",
    "    return mus, nus, Sigmas, Taus, UserToU, UserToV, JokeToU, JokeToV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we try to code up Part 3!\n",
    "def LogLikelihood(theta, rij, k, sigmaepsilon2, z1, z2):\n",
    "    # Return the log-likelihood of a single rating.\n",
    "    # Assumes rij | ui, vj \\sim N(ui^Tvj, sigamepsilon2)\n",
    "    assert(len(theta) == k * 4)\n",
    "    mui, nuj, logsigmai, logtauj = theta[:k], theta[k:2*k], theta[2*k:3*k], theta[3*k:]\n",
    "    mean = 0\n",
    "    for a in range(k):\n",
    "        mean = mean + (mui[a] + z1[a] * np.exp(logsigmai[a])) * (nuj[a] + z2[a] * np.exp(logtauj[a]))\n",
    "    \n",
    "    mean = mean / k\n",
    "    return N1.logpdf(rij, mean, sigmaepsilon2)\n",
    "\n",
    "gradLogLikelihood = grad(LogLikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MCExpectedGradLogLikelihood(theta, rij, sigmaepsilon2, K, S):\n",
    "    # S is the number of samples to use for estimating the expected value of the gradient of the log likelihood\n",
    "    # K is number of latent variables\n",
    "    total = np.zeros(len(theta))\n",
    "    for i in range(S):\n",
    "        z1, z2 = N(np.zeros(K), np.identity(K)), N(np.zeros(K), np.identity(K))\n",
    "        total = total + gradLogLikelihood(theta, rij, K, sigmaepsilon2, z1, z2)\n",
    "    return total / S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now code up our gradient ascept algorithm, on the data, using multiple epochs!\n",
    "EPOCHS = 10\n",
    "sigmaU2, sigmaV2, sigmaepsilon2 = 5.0, 5.0, 1.0\n",
    "def optimzeLowerBound(alpha, K, S, NSamples):\n",
    "    # Optimizes the lowerbound and the set of parameters for each epoch as muis, nuis, logsigmais, logtauis\n",
    "    # K is the number of latent variables\n",
    "    # alpha is the learning rate for stochastic gradient ascent.\n",
    "    # Initialize parameters\n",
    "    print \"Initializing initial distributions...\"\n",
    "    mus, nus, Sigmas, Taus, UserToU, UserToV, JokeToU, JokeToV = createInitialParams(\n",
    "        K,sigmaU2, sigmaV2, trainUserIDs, trainJokeIDs, trainUToJ, trainJToU)\n",
    "    \n",
    "    # Convert to logs\n",
    "    logSigmas = {k: -0.5 * np.log(np.diag(v)) for k,v in Sigmas.iteritems()}\n",
    "    logTaus = {k : -0.5 * np.log(np.diag(v)) for k,v in Taus.iteritems()}\n",
    "    \n",
    "    # Let's precompute numbers\n",
    "    numUsers = {joke : len(JokeToU[joke]) for joke in trainJokeIDs} \n",
    "    numJokes = {user : len(UserToV[user]) for user in trainUserIDs} \n",
    "    \n",
    "    print \"Initialized prior distributions...\"\n",
    "    \n",
    "    print \"Initializing random samples of params...\"\n",
    "    thetasUser = np.zeros((len(trainUserIDs), 2*K))\n",
    "    thetasJoke = np.zeros((len(trainJokeIDs), 2*K))\n",
    "    for user in trainUserIDs:\n",
    "        thetasUser[UserToU[user], :] = np.concatenate((mus[user], logSigmas[user]))\n",
    "    for joke in trainJokeIDs:\n",
    "        thetasJoke[JokeToV[joke], :] = np.concatenate((nus[joke], logTaus[joke]))\n",
    "        \n",
    "    userThetas = {}\n",
    "    jokeThetas = {}\n",
    "    userThetas[0] = np.copy(thetasUser)\n",
    "    jokeThetas[0] = np.copy(thetasJoke)\n",
    "    print \"Finished initializing parameters...\"\n",
    "        \n",
    "    print \"Initializing epoch iterations!....\"    \n",
    "    for epoch in range(EPOCHS):\n",
    "        userThetas[epoch + 1] = np.copy(userThetas[epoch])\n",
    "        jokeThetas[epoch + 1] = np.copy(jokeThetas[epoch])\n",
    "        \n",
    "        # select a random subset of the training data!\n",
    "        rows = random.sample(train.index, NSamples)\n",
    "        trainSample = train.ix[rows]\n",
    "        i = 0\n",
    "        for rating_Num,row in trainSample.iterrows():\n",
    "            # Get the rating and user for this row\n",
    "            rij = row.Rating\n",
    "            user = row.UserID\n",
    "            joke = row.JokeID\n",
    "            \n",
    "            # Grab parameters for this user in the epoch we're updating!\n",
    "            mui = userThetas[epoch+1][UserToU[user], :K]\n",
    "            logsigmai = userThetas[epoch+1][UserToU[user], K:]\n",
    "            nuj = jokeThetas[epoch+1][JokeToV[joke], :K]\n",
    "            logtauj = jokeThetas[epoch+1][JokeToV[joke], K:]\n",
    "            \n",
    "            # Calculate gradient of likelihood term for mui, nuj, logsigmai, logtauj.\n",
    "            theta = np.concatenate((mui, nuj, logsigmai, logtauj))\n",
    "            assert(len(theta) == 4 * K)\n",
    "            gradL = MCExpectedGradLogLikelihood(theta, rij, sigmaepsilon2, K, S)\n",
    "            \n",
    "            # Calculate analytical gradient of prior and entropy terms, and additionally, scale them!\n",
    "            gradKL = np.concatenate((-mui / (float(sigmaU2) * numJokes[user]), \n",
    "                                     -nuj / (float(sigmaV2) * numUsers[joke]),\n",
    "                                     np.exp(2 * logsigmai) / (float(sigmaU2) * numJokes[user]),\n",
    "                                     np.exp(2 * logtauj) / (float(sigmaV2) * numUsers[joke])))\n",
    "            \n",
    "            # Now we can combine them\n",
    "            gradient = gradL + gradKL \n",
    "            if np.isnan(gradKL).any():\n",
    "                gradient = gradL\n",
    "                \n",
    "            if np.isnan(gradient).any():\n",
    "                pass\n",
    "            else:\n",
    "                # Update the results using gradient ascent!\n",
    "                userThetas[epoch+1][UserToU[user], :K] += alpha * (gradient[:K])\n",
    "                jokeThetas[epoch+1][JokeToV[joke], :K] += alpha * (gradient[K:2*K])\n",
    "                userThetas[epoch+1][UserToU[user], K:] += alpha * (gradient[2*K:3*K])\n",
    "                jokeThetas[epoch+1][JokeToV[joke], K:] += alpha * (gradient[3*K:])\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print \"Processed {} ratings\".format(i)\n",
    "            i += 1\n",
    "                       \n",
    "        print \"Finished the results for epoch {}\".format(epoch)\n",
    "            \n",
    "    print \"Finished Epochs!\"\n",
    "    \n",
    "    return userThetas, jokeThetas, UserToU, UserToV, JokeToU, JokeToV\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createRatingSet(data, nusers, njokes, UserToU, JokeToV, filename='test.out'):\n",
    "    '''\n",
    "    Given a pandas data frame, creates a matrix R with the ratings of each user filled in.\n",
    "    '''\n",
    "    try:\n",
    "        R = np.loadtxt(filename)\n",
    "    except IOError:\n",
    "        R = np.zeros((nusers, njokes))\n",
    "        ignored = 0\n",
    "        for i, row in data.iterrows():\n",
    "            try:\n",
    "                i = UserToU[row.UserID]\n",
    "                j = JokeToV[row.JokeID]\n",
    "                R[i,j] = row.Rating\n",
    "            except KeyError:\n",
    "                # We don't have this user or joke in our training set, so we just ignore it?\n",
    "                ignored += 1\n",
    "                pass\n",
    "            \n",
    "        print \"Ignored a total of {} users/ratings that don't exist in training set\".format(ignored)\n",
    "\n",
    "        np.savetxt(filename, R)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nonZeroSubtract(x,y):\n",
    "    '''\n",
    "    Computs x-y iff x \\neq 0, otherwise returns 0.\n",
    "    '''\n",
    "    return x-y if x != 0 else 0\n",
    "\n",
    "vectSub = np.vectorize(nonZeroSubtract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can calculate these after as long as we return our stack of Us \n",
    "def getTrainTestResults(userTheta, jokeTheta, UserToU, JokeToV,k, nsamples = 100):\n",
    "    '''\n",
    "    Given the sequence of created parameters from optimizing our lowerbound, we now use those parameters\n",
    "    to sample from the distributution nsamples. \n",
    "    '''\n",
    "    # We calculate the results profressively, so let us iterated over the sorted keys for the epochs.\n",
    "    assert(sorted(userTheta.keys()) == sorted(jokeTheta.keys()))\n",
    "    # The running mean for U and V\n",
    "    n, m = len(trainUserIDs), len(trainJokeIDs)\n",
    "\n",
    "    # Precompute the users/joke that exists in the testing set\n",
    "    testUserUIndex = [UserToU[user] for user in testUserIDs if user in UserToU]\n",
    "    testJokeVIndex = [JokeToV[joke] for joke in testJokeIDs if joke in JokeToV]\n",
    "    \n",
    "    # Create R (note that this function tries to read from disk if R already exists)\n",
    "    RTrain = createRatingSet(train, n, m, UserToU, JokeToV, 'R_Train.out')\n",
    "    RTest = createRatingSet(test, n, m, UserToU, JokeToV, 'R_Test.out')\n",
    "    \n",
    "    assert(np.count_nonzero(RTrain) == len(train))\n",
    "    trainN = np.count_nonzero(RTrain)\n",
    "    print \"Training samples: {}.\".format(trainN)\n",
    "    testN = np.count_nonzero(RTest)\n",
    "    print \"Testing samples: {}.\".format(testN)\n",
    "    \n",
    "    train_likelihood = []\n",
    "    train_l = []\n",
    "    test_likelihood = []\n",
    "    test_l = []\n",
    "    print \"Beginning epochs!\"\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Sample so we can estimate the likelihood term!\n",
    "        tll = 0\n",
    "        tll_test = 0\n",
    "        \n",
    "        print \"Starting sampling from distribution!\"\n",
    "        for _ in range(nsamples):\n",
    "            # Sample U,V\n",
    "            U = np.zeros((n,k))\n",
    "            V = np.zeros((m,k))\n",
    "            for user in trainUserIDs:\n",
    "                index = UserToU[user]\n",
    "                U[index, :] = N(userTheta[epoch][index, :k], np.diag(np.exp(2 * userTheta[epoch][index, k:])))\n",
    "            for joke in trainJokeIDs:\n",
    "                index = JokeToV[joke]\n",
    "                V[index, :] = N(jokeTheta[epoch][index, :k], np.diag(np.exp(2 * jokeTheta[epoch][index, k:])))\n",
    "                \n",
    "            # We have a new sample, so calculate predictions\n",
    "            predictions = np.dot(U,V.T)\n",
    "            \n",
    "            # Calculate likelihood\n",
    "            tll += -0.5 * np.log(2 * math.pi * sigmaepsilon2) - 0.5 * np.sum(vectSub(RTrain, predictions)**2)\n",
    "            tll_test += -0.5 * np.log(2 * math.pi * sigmaepsilon2) - 0.5 * np.sum(vectSub(RTest, predictions)**2)\n",
    "        \n",
    "        print \"Finished sampling! Calculating values!\"\n",
    "        \n",
    "        tll = tll / float(nsamples)\n",
    "        print \"Train Log Likelihood for Epoch {} is {}\".format(epoch, tll)\n",
    "        tll_test = tll_test / float(nsamples)\n",
    "        print \"Test Log Likelihood for Epoch {} is {}\".format(epoch, tll_test)\n",
    "        \n",
    "        # Now that we have the expected likelihood, use closed form solution to calculate entropy and prior terms\n",
    "        # for the lower bound!\n",
    "        train_bound = tll + trainN * k * (1 - 0.5 * np.log(sigmaU2) - 0.5 * np.log(sigmaV2))\n",
    "        train_bound -= np.sum(np.exp(2 * userTheta[epoch][:, k:]) \n",
    "                              + userTheta[epoch][:, :k] **2 / float(2 * sigmaU2) - userTheta[epoch][:, k:])\n",
    "        train_bound -= np.sum(np.exp(2 * jokeTheta[epoch][:, k:]) \n",
    "                              + jokeTheta[epoch][:, :k] **2 / float(2 * sigmaU2) - jokeTheta[epoch][:, k:])\n",
    "        print \"Train Lower Bound for Epoch {} is {}\".format(epoch, train_bound)\n",
    "        \n",
    "        # For the test, we repeat the same as the train but we can only do it with the parameters where\n",
    "        # the user/joke exists in the test set.\n",
    "        test_bound = tll_test + testN * k * (1 - 0.5 * np.log(sigmaU2) - 0.5 * np.log(sigmaV2))\n",
    "        test_bound -= np.sum(np.exp(2 * userTheta[epoch][testUserUIndex, k:]) \n",
    "                             + userTheta[epoch][testUserUIndex, :k] **2 / float(2 * sigmaU2) \n",
    "                             - userTheta[epoch][testUserUIndex, k:])\n",
    "        test_bound -= np.sum(np.exp(2 * jokeTheta[epoch][testJokeVIndex, k:]) \n",
    "                             + jokeTheta[epoch][testJokeVIndex, :k] **2 / float(2 * sigmaU2) \n",
    "                             - jokeTheta[epoch][testJokeVIndex, k:])\n",
    "        print \"Test Lower Bound for Epoch {} is {}\".format(epoch, test_bound)\n",
    "        \n",
    "        # Store the results!\n",
    "        train_likelihood.append(tll)\n",
    "        train_l.append(train_bound)\n",
    "        test_likelihood.append(tll_test)\n",
    "        test_l.append(test_bound)\n",
    "        \n",
    "    return train_likelihood, train_l, test_likelihood, test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(userThetas[0][0, :1], np.diag(np.exp(2 * userThetas[0][0, 1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ll_k, train_lb_k, test_ll_k, test_lb_k = [], [], [], []\n",
    "for a in range(1,5):\n",
    "    userThetas, jokeThetas, UserToU, UserToV, JokeToU, JokeToV = optimzeLowerBound(0.1, a, 1, 1)\n",
    "    train_ll , train_lb, test_ll, test_lb = getTrainTestResults(userThetas, jokeThetas, UserToU, JokeToV, a, 10)\n",
    "    train_ll_k.append(train_ll)\n",
    "    train_lb_k.append(train_lb)\n",
    "    test_ll_k.append(test_ll)\n",
    "    test_lb_k.append(test_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_lb_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(10), train_lb_k[0], color='b', label=\"Train Data LowerBound\")\n",
    "plt.scatter(range(10), train_ll_k[0], color='r', label=\"Train Data Log Likelihood\" )\n",
    "plt.scatter(range(10), test_lb_k[0], color='g', label=\"Test Data Lower Bound\")\n",
    "plt.scatter(range(10), test_ll_k[0], color='y', label=\"Test Data Log Likelihood\")\n",
    "plt.xlim((0,10))\n",
    "plt.title(\"Log Likelihood and LowerBound Plots for K = 1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Log Likelihood/LowerBound\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
