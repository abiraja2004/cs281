{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import pprint\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.loadtxt(\"CASP.csv\", delimiter = \",\", skiprows = 1)\n",
    "y = data[ : , 0 ]\n",
    "X = data[ : , 1 : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to split the data into first fraction_train% and remaining as test.\n",
    "def split_train_test(X, y, fraction_train = 9.0 / 10.0):\n",
    "    end_train = round(X.shape[ 0 ] * fraction_train)\n",
    "    X_train = X[ 0 : end_train, ]\n",
    "    y_train = y[ 0 : end_train ]\n",
    "    X_test = X[ end_train :, ]\n",
    "    y_test = y[ end_train : ]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Given a test and feature set, normalize each so mean is 0 and unit deviation.\n",
    "def normalize_features(X_train, X_test):\n",
    "    mean_X_train = np.mean(X_train, 0)\n",
    "    std_X_train = np.std(X_train, 0)\n",
    "    std_X_train[ std_X_train == 0 ] = 1\n",
    "    X_train_normalized = (X_train - mean_X_train) / std_X_train\n",
    "    X_test_normalized = (X_test - mean_X_train) / std_X_train\n",
    "    return X_train_normalized, X_test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:4: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:5: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:6: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:7: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "# Clean data\n",
    "X_train, y_train, X_test, y_test = split_train_test(X, y)\n",
    "X_train, X_test = normalize_features(X_train, X_test)\n",
    "X_train = np.concatenate((X_train, np.ones((X_train.shape[ 0 ], 1))), 1)\n",
    "X_test = np.concatenate((X_test, np.ones((X_test.shape[ 0 ], 1))), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function which finds the MAP estimate.\n",
    "def mapEstimate(Xtrain, Ytrain, lam=0):\n",
    "    \"\"\"\n",
    "    Ridge regression performed on the given design matrix (Xtrain) and results (Ytrain). \n",
    "    Essentially, we implement the algorithm described in 7.5.2 of Murphy.\n",
    "    \n",
    "    lam is lambda=sigma^2/tau^2 where sigma is variance of data and tau is prior variance.\n",
    "    \n",
    "    Returns the mapEstimate of the paramters w.\n",
    "    \"\"\"\n",
    "    (N,D) = Xtrain.shape\n",
    "    Ny = Ytrain.shape\n",
    "    assert(N == Ny[0])\n",
    "    \n",
    "    # Attach additional prior data to Xtrain.\n",
    "    priorData = np.diag([np.sqrt(lam) for _ in xrange(D)])\n",
    "    Xtrainp = np.concatenate((Xtrain, priorData))\n",
    "    Ytrainp = np.concatenate((Ytrain, np.zeros(D)))\n",
    "    \n",
    "    # Find QR decomposition of Xp to avoid inverse.\n",
    "    Q,R = np.linalg.qr(Xtrainp)\n",
    "    inverseR = np.linalg.inv(R)\n",
    "    return inverseR.dot(Q.T).dot(Ytrainp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate MAP estimate of W\n",
    "variance = 1.0\n",
    "priorVariance = (.1)**2\n",
    "wMAP = mapEstimate(X_train, y_train, lam=variance / priorVariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 3.15915374,  2.45711946,  1.00708615, -5.70882509,  0.19052137,\n",
      "       -1.49631801, -0.25545196,  0.82089849, -0.61340907,  7.72464619])\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(wMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict using model parameters W\n",
    "def predict(X,w):\n",
    "    return X.dot(w)\n",
    "\n",
    "# Calculate root mean square error\n",
    "def RMS(pred, target):\n",
    "    return np.sqrt(((pred - target) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.21279653606\n"
     ]
    }
   ],
   "source": [
    "# Actually print the RMSE\n",
    "print RMS(predict(X_test,wMAP), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate the gradiant of the posterior on W.\n",
    "def gradiantPosterior(w, X, Y, lam2, tau2):\n",
    "    return 1.0/lam2 * X.T.dot(Y - X.dot(w)) - 1.0/tau2 * w\n",
    "\n",
    "def gradiant(w):\n",
    "    return gradiantPosterior(w, X_train, y_train, variance, priorVariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluates logPosterior up to an additive constant.\n",
    "def logPosterior(w, X, Y, lam2, tau2):\n",
    "    (N,D) = X.shape\n",
    "    var0 = -(1. /(2 * lam2)) * (Y - X.dot(w)).T.dot(Y - X.dot(w))\n",
    "    var1 = -(1. / (2 * tau2)) * w.T.dot(w)\n",
    "    return var0 + var1\n",
    "def logDist(w):\n",
    "    return logPosterior(w, X_train, y_train, variance, priorVariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj_and_gradiant(w):\n",
    "    return (logDist(w), gradiant(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def verifyGradiant(x, epsilon = 0.0001):\n",
    "    \"\"\"\n",
    "    Uses finite differences to verify the gradiant of our log posterior.\n",
    "    \"\"\"\n",
    "    units = np.diag([epsilon for _ in x])\n",
    "    grad = gradiant(x)\n",
    "    finiteDiff = [(logDist(x + units[i]) - logDist(x - units[i])) / (2*epsilon) for i in xrange(len(x))]\n",
    "    return all([abs(g - f) < 0.01 for g,f in zip(grad, finiteDiff)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Verify the result for ntrials\n",
    "def verifyGradiantM(ntrials):\n",
    "    xs = [np.random.rand(D) for _ in xrange(ntrials)]\n",
    "    return all(map(verifyGradiant, xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print verifyGradiantM(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimizePosterior(D):\n",
    "    return optimize.minimize(lambda x: (-1*logDist(x),gradiant(x)), np.zeros(D),method=\"L-BFGS-B\",\n",
    "                             jac=True, options={'maxiter': 100, 'disp' : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimizePosteriorGeneral(D,X,Y,lam2,tau2):\n",
    "    return optimize.minimize(lambda x: (-1*logPosterior(x,X,Y,lam2,tau2), gradiantPosterior(x,X,Y,lam2,tau2)),\n",
    "                             np.zeros(D),method=\"L-BFGS-B\", jac=True, options={'maxiter': 100, 'disp' : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 3.15915374  2.45711946  1.00708615 -5.70882509  0.19052137 -1.49631801\n",
      " -0.25545196  0.82089849 -0.61340907  7.72464619]\n"
     ]
    }
   ],
   "source": [
    "wMin = minimizePosterior(wMAP.shape[0]).x\n",
    "print wMin, wMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.92540584064\n"
     ]
    }
   ],
   "source": [
    "# Actually print the RMSE of this new estimate\n",
    "print RMS(predict(X_test,wMin), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.1152647488436624"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a target dimension d and an input dimension D of the features,\n",
    "# constructs random non-linear functions to take each data point from d -> D dimensions.\n",
    "def genNonLinearFunctions(d,D):\n",
    "    A = np.random.multivariate_normal(np.zeros(D),np.identity(D), d)\n",
    "    b = np.random.uniform(high=2*math.pi, size=(d,1))\n",
    "    return lambda x : np.cos(A.dot(x) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvals = [100, 200, 400, 600]\n",
    "(N,D) = X_train.shape\n",
    "funs = [genNonLinearFunctions(d, D) for d in dvals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDesignMatrices = []\n",
    "for f in funs:\n",
    "    # The new N x d design matrices.\n",
    "    newX_train, newX_test = f(X_train.T).T, f(X_test.T).T\n",
    "    \n",
    "    # We renormalize in the new space\n",
    "    newX_train, newX_test = normalize_features(newX_train, newX_test)\n",
    "    \n",
    "    # We append a bias term.\n",
    "    newX_train = np.concatenate((newX_train, np.ones((newX_train.shape[ 0 ], 1))), 1)\n",
    "    newX_test = np.concatenate((newX_test, np.ones((newX_test.shape[ 0 ], 1))), 1)\n",
    "    \n",
    "    # Append to our list so we don't recalculate in the future\n",
    "    newDesignMatrices.append((newX_train, newX_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now record timing and results in a dictionary.\n",
    "results = []\n",
    "for (i,d) in enumerate(dvals):\n",
    "    result = {'d' : d }\n",
    "    \n",
    "    # QR Decomp\n",
    "    start = time.time()\n",
    "    train, test = newDesignMatrices[i]\n",
    "    wMAP = mapEstimate(train, y_train, lam=variance / priorVariance)\n",
    "    result['qr_time'] = time.time() - start\n",
    "    error = RMS(predict(test, wMAP), y_test)\n",
    "    result['qr_error'] = error\n",
    "    \n",
    "    # L-BFGS\n",
    "    start = time.time()\n",
    "    wMin = minimizePosteriorGeneral(wMAP.shape[0], train, y_train, variance, priorVariance).x\n",
    "    lbfgs = time.time() - start\n",
    "    result['lbgfs_time'] = time.time() - start\n",
    "    error = RMS(predict(test, wMin), y_test)\n",
    "    result['lbgfs_error'] = error\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now plot the results.\n",
    "qrx, qry, lbgfsx, lbgfsy = [], [], [], []\n",
    "for result in results:\n",
    "    qrx.append(result['qr_time']) \n",
    "    qry.append(result['qr_error'])\n",
    "    lbgfsx.append(result['lbgfs_time'])\n",
    "    lbgfsy.append(result['lbgfs_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qrplot = plt.scatter(qrx, qry, color=\"red\")\n",
    "# Anotetate the points with their respective d-values\n",
    "for i in range(len(dvals)):\n",
    "    plt.annotate(\"d={}\".format(dvals[i]), (qrx[i],qry[i]), textcoords = 'offset points', ha = 'right', \n",
    "                 va = 'bottom')\n",
    "lbgsfsplot = plt.scatter(lbgfsx, lbgfsy, color=\"blue\")\n",
    "  \n",
    "plt.xlabel(\"Execution Time (s)\")\n",
    "plt.ylabel(\"Root Mean Square Error\")\n",
    "plt.title(\"RMSE vs Time for Least Square Ridge Regression\")\n",
    "plt.legend((qrplot, lbgsfsplot),\n",
    "           ('QR Decomposition Method', 'Maximum A Posteriori'),\n",
    "           scatterpoints=1,\n",
    "           loc='top right',\n",
    "           ncol=2,\n",
    "           fontsize=10)\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
