{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import autograd.numpy as np  # Thinly-wrapped numpy\n",
    "from autograd import grad \n",
    "from autograd import elementwise_grad\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import multivariate_normal as N\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "# Each row is formatted as [User ID] [Joke ID] [Rating]\n",
    "ratingsFile =  \"ratings.dat\"\n",
    "allData = pd.read_csv(ratingsFile, sep=\" \", header=None)\n",
    "allData = allData.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allData.columns = ['UserID', 'JokeID', 'Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitData(R, trainPercent=0.95):\n",
    "    # R is an NxM matrix where r_ij is the rating assigned by user i to jokes.\n",
    "    # Here, we split the data into matrices of size ratings x 2, where the two columns are JokeID and Rating\n",
    "    N,M = R.shape\n",
    "    totalRatings = np.count_nonzero(R)\n",
    "    testMat = np.zeros((totalRatings,2)) \n",
    "    trainMat = np.zeros((totalRatings, 2))\n",
    "    trainN, testN = 0,0\n",
    "    print N,M\n",
    "    for j in range(M):\n",
    "        tratings = float(np.count_nonzero(R[:,j]))\n",
    "        inTraining = 0\n",
    "        for i in range(N):\n",
    "            if R[i,j] > 0:\n",
    "                if inTraining <= (tratings * trainPercent):\n",
    "                    inTraining += 1\n",
    "                    trainMat[trainN, :] = np.array([j, R[i,j]])\n",
    "                    trainN += 1\n",
    "                else:\n",
    "                    testMat[testN, :] = np.array([j, R[i,j]])\n",
    "                    testN += 1\n",
    "    # Note that the results are zero indexed! We've mofidied the joke ids so now they are 0-indexed!!!!!\n",
    "    return trainMat, testMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# by user i to joke j\n",
    "R = np.zeros((max(allData.UserID), max(allData.JokeID)))\n",
    "for row in allData.iterrows():\n",
    "    r = row[1]\n",
    "    R[r['UserID']-1, r['JokeID']-1] = r['Rating']\n",
    "# np.savetxt('all_ratings.txt', R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R = np.loadtxt('all_ratings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978 150\n"
     ]
    }
   ],
   "source": [
    "train, test = splitData(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop all 0s from the data\n",
    "def dropZeroRows(a):\n",
    "    mask = np.all(np.isnan(a) | np.equal(a, 0), axis=1)\n",
    "    return a[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainE, testE = dropZeroRows(train), dropZeroRows(test)\n",
    "trainE, testE = trainE.astype(int), testE.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('95trainE.txt', trainE)\n",
    "np.savetxt('5testE.txt', testE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1761439, 1761439)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainE)+len(testE), len(allData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autograd.scipy.stats import norm\n",
    "import sys\n",
    "def log_lik(weights, x_i, br1, br2):\n",
    "    # br1 and br2 is a vector of respecive rankings.\n",
    "    # We add the np.sum to vectorize the below into batches. Note that x_i can now be a matrix\n",
    "    # print weights.shape, x_i.shape, br1.shape, br2.shape\n",
    "    cdf = lambda y: norm.cdf( (y - np.dot(weights[:-1], np.transpose(x_i)))/weights[-1])\n",
    "    return np.log(cdf(br2) - cdf(br1))\n",
    "\n",
    "def log_prior(w):\n",
    "    return np.sum(norm.logpdf(w))\n",
    "\n",
    "def scaled_posterior(weights, x_i, br1, br2):\n",
    "    # Same parameters as log_lik.\n",
    "    # Note that we add up all the results to be able to batch process the final posterior probability\n",
    "    return np.sum(log_lik(weights, x_i, br1, br2) + log_prior(weights[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posteriorGradient = grad(scaled_posterior, argnum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the feature space matrix\n",
    "# Solution for sigma^2 is 16, or roughly 16\n",
    "X = np.loadtxt('X.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 151)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam(grad, x, num_iters=100,\n",
    "         step_size=0.001, b1=0.9, b2=0.999, eps=10**-8):\n",
    "    \"\"\"\n",
    "    Adam as described in http://arxiv.org/pdf/1412.6980.pdf.\n",
    "    It's basically RMSprop with momentum and some correction terms.\n",
    "    Code directly copied from autograd package and modified slightly,\n",
    "    as it now maximizes rather than minimizes. \n",
    "    \"\"\"\n",
    "    m = np.zeros(len(x))\n",
    "    v = np.zeros(len(x))\n",
    "    for i in range(num_iters):\n",
    "        g = grad(x)\n",
    "        m = (1 - b1) * g      + b1 * m  # First  moment estimate.\n",
    "        v = (1 - b2) * (g**2) + b2 * v  # Second moment estimate.\n",
    "        mhat = m / (1 - b1**(i + 1))    # Bias correction.\n",
    "        vhat = v / (1 - b2**(i + 1))\n",
    "        x += step_size*mhat/(np.sqrt(vhat) + eps)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = [-sys.maxint/10, -4, -2, 2, 4, sys.maxint/10]\n",
    "def ratingToLowerBound(r):\n",
    "    # Takes in a rating and returns br1, the lower bound in our intervals\n",
    "    # Note that ratings are 1 indexed\n",
    "    return b[r-1]\n",
    "    \n",
    "def ratingToUpperBound(r):\n",
    "    return b[r]\n",
    "\n",
    "lowVect = np.vectorize(ratingToLowerBound)\n",
    "highVect= np.vectorize(ratingToUpperBound)\n",
    "\n",
    "def stochasticGradientDescent(T, data, batchSize):\n",
    "    (n, m) = data.shape\n",
    "    #print (n,m)\n",
    "    w0 = N(mean=np.zeros(X.shape[1] + 1),cov=np.identity(X.shape[1] + 1))\n",
    "    #print w0.shape\n",
    "    # Precomput br1 and br2 values\n",
    "    br1all = lowVect(data[:, 1])\n",
    "    br2all = highVect(data[:, 1])\n",
    "    for epoch in range(T):\n",
    "        # Iterate over the data size with epochs\n",
    "        for i in xrange(0, len(data), batchSize):\n",
    "            batchJokeIDs = data[i:min(len(data),i+batchSize), 0]\n",
    "            ratings = data[i:min(len(data),i+batchSize), 1]\n",
    "            xi = X[batchJokeIDs, :]\n",
    "            br1 = br1all[i:min(len(data),i+batchSize)]\n",
    "            br2 = br2all[i:min(len(data),i+batchSize)]\n",
    "            w0 = adam(lambda w: posteriorGradient(w0, xi, br1, br2), w0)\n",
    "            if i % batchSize == 0:\n",
    "                print \"A total of {} points have been completed.\".format(i)\n",
    "        print \"Finished epoch {}\".format(epoch)\n",
    "            \n",
    "    return w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1673440"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 0 points have been completed.\n",
      "A total of 100000 points have been completed."
     ]
    }
   ],
   "source": [
    "w = stochasticGradientDescent(10, trainE, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wFinal = np.array([  4.22455732e-02,   4.22473088e-02,   5.37630673e-06,\n",
    "         4.22455732e-02,   5.37596623e-06,   4.22473088e-02,\n",
    "         4.22455732e-02,   4.22473088e-02,   4.22473221e-02,\n",
    "         4.22473222e-02,   4.22455732e-02,  -2.54292998e-06,\n",
    "         4.22455696e-02,   4.22455732e-02,   5.37532906e-06,\n",
    "         5.37634719e-06,  -5.21942825e-06,  -5.21937071e-06,\n",
    "        -5.21942778e-06,   4.22455732e-02,   5.37634719e-06,\n",
    "         5.37634675e-06,   5.37634719e-06,   4.22455696e-02,\n",
    "         4.22473086e-02,   5.21942780e-06,   5.37533456e-06,\n",
    "         4.22455732e-02,   5.37533405e-06,  -2.78179336e-06,\n",
    "         4.22455732e-02,  -5.21942878e-06,   5.21229106e-06,\n",
    "        -5.21943212e-06,  -2.78179336e-06,   4.22473222e-02,\n",
    "         4.22455732e-02,   4.22455696e-02,   5.21229121e-06,\n",
    "        -2.78179361e-06,   4.22455732e-02,  -2.40925210e-06,\n",
    "        -5.21942778e-06,   5.37630675e-06,  -5.21942825e-06,\n",
    "         4.22455696e-02,   4.22455696e-02,  -2.78179294e-06,\n",
    "         4.22455696e-02,  -5.21942748e-06,   5.21229106e-06,\n",
    "         5.37533445e-06,  -2.78179294e-06,  -5.21942748e-06,\n",
    "         4.22455696e-02,   5.21229209e-06,  -5.21942878e-06,\n",
    "         5.37533405e-06,  -2.78397019e-06,  -5.21942778e-06,\n",
    "        -5.21942825e-06,   5.37533445e-06,  -5.21942748e-06,\n",
    "        -5.21942778e-06,   4.22455732e-02,   4.22473086e-02,\n",
    "        -5.21942748e-06,  -5.21942878e-06,  -5.21942778e-06,\n",
    "         5.21942778e-06,   5.37533456e-06,   5.21220933e-06,\n",
    "        -5.21943212e-06,  -2.78396471e-06,   5.21303725e-06,\n",
    "         5.21229121e-06,   5.37532905e-06,  -2.40932524e-06,\n",
    "         4.22455732e-02,  -5.21942778e-06,   4.22455696e-02,\n",
    "         5.21942778e-06,   5.37532905e-06,  -5.21943155e-06,\n",
    "         4.22455696e-02,   4.22455696e-02,  -5.21942778e-06,\n",
    "         5.21229106e-06,   5.21942778e-06,   5.21942778e-06,\n",
    "         5.21942778e-06,  -5.21942878e-06,   5.21942780e-06,\n",
    "        -5.21942748e-06,   5.21942778e-06,   5.21942780e-06,\n",
    "        -5.21942878e-06,   5.21229121e-06,  -5.21942878e-06,\n",
    "         4.22455696e-02,   5.21942778e-06,   5.21942778e-06,\n",
    "         5.21229209e-06,  -5.21942778e-06,   4.22455696e-02,\n",
    "         5.21942778e-06,   5.37532905e-06,  -5.21942748e-06,\n",
    "        -5.21942878e-06,   5.21229112e-06,   5.21942778e-06,\n",
    "        -5.21942778e-06,   5.21229106e-06,   4.22455696e-02,\n",
    "         5.21942778e-06,  -5.21942778e-06,   5.21942778e-06,\n",
    "        -5.21943210e-06,  -5.21942878e-06,   4.22455696e-02,\n",
    "        -5.21936489e-06,   5.21942778e-06,  -5.21942778e-06,\n",
    "         5.21229121e-06,  -5.21942778e-06,   5.21942780e-06,\n",
    "        -5.21942878e-06,   4.22455696e-02,  -5.21942778e-06,\n",
    "        -5.21942878e-06,   5.21942778e-06,   4.22455696e-02,\n",
    "         5.21942778e-06,   5.37532905e-06,   5.21942778e-06,\n",
    "         5.37533445e-06,  -5.21942778e-06,  -5.21942778e-06,\n",
    "         5.21942778e-06,   5.37532905e-06,   5.37532905e-06,\n",
    "         5.21942778e-06,   5.21942780e-06,   5.21942780e-06,\n",
    "         4.22455696e-02,   5.21942778e-06,   5.37532905e-06,\n",
    "        -5.21936392e-06,   5.21942778e-06,   5.21942778e-06,\n",
    "         4.22473088e-02,  -3.91020096e+00])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.22455732e-02,   4.22473088e-02,   5.37630673e-06,\n",
       "         4.22455732e-02,   5.37596623e-06,   4.22473088e-02,\n",
       "         4.22455732e-02,   4.22473088e-02,   4.22473221e-02,\n",
       "         4.22473222e-02,   4.22455732e-02,  -2.54292998e-06,\n",
       "         4.22455696e-02,   4.22455732e-02,   5.37532906e-06,\n",
       "         5.37634719e-06,  -5.21942825e-06,  -5.21937071e-06,\n",
       "        -5.21942778e-06,   4.22455732e-02,   5.37634719e-06,\n",
       "         5.37634675e-06,   5.37634719e-06,   4.22455696e-02,\n",
       "         4.22473086e-02,   5.21942780e-06,   5.37533456e-06,\n",
       "         4.22455732e-02,   5.37533405e-06,  -2.78179336e-06,\n",
       "         4.22455732e-02,  -5.21942878e-06,   5.21229106e-06,\n",
       "        -5.21943212e-06,  -2.78179336e-06,   4.22473222e-02,\n",
       "         4.22455732e-02,   4.22455696e-02,   5.21229121e-06,\n",
       "        -2.78179361e-06,   4.22455732e-02,  -2.40925210e-06,\n",
       "        -5.21942778e-06,   5.37630675e-06,  -5.21942825e-06,\n",
       "         4.22455696e-02,   4.22455696e-02,  -2.78179294e-06,\n",
       "         4.22455696e-02,  -5.21942748e-06,   5.21229106e-06,\n",
       "         5.37533445e-06,  -2.78179294e-06,  -5.21942748e-06,\n",
       "         4.22455696e-02,   5.21229209e-06,  -5.21942878e-06,\n",
       "         5.37533405e-06,  -2.78397019e-06,  -5.21942778e-06,\n",
       "        -5.21942825e-06,   5.37533445e-06,  -5.21942748e-06,\n",
       "        -5.21942778e-06,   4.22455732e-02,   4.22473086e-02,\n",
       "        -5.21942748e-06,  -5.21942878e-06,  -5.21942778e-06,\n",
       "         5.21942778e-06,   5.37533456e-06,   5.21220933e-06,\n",
       "        -5.21943212e-06,  -2.78396471e-06,   5.21303725e-06,\n",
       "         5.21229121e-06,   5.37532905e-06,  -2.40932524e-06,\n",
       "         4.22455732e-02,  -5.21942778e-06,   4.22455696e-02,\n",
       "         5.21942778e-06,   5.37532905e-06,  -5.21943155e-06,\n",
       "         4.22455696e-02,   4.22455696e-02,  -5.21942778e-06,\n",
       "         5.21229106e-06,   5.21942778e-06,   5.21942778e-06,\n",
       "         5.21942778e-06,  -5.21942878e-06,   5.21942780e-06,\n",
       "        -5.21942748e-06,   5.21942778e-06,   5.21942780e-06,\n",
       "        -5.21942878e-06,   5.21229121e-06,  -5.21942878e-06,\n",
       "         4.22455696e-02,   5.21942778e-06,   5.21942778e-06,\n",
       "         5.21229209e-06,  -5.21942778e-06,   4.22455696e-02,\n",
       "         5.21942778e-06,   5.37532905e-06,  -5.21942748e-06,\n",
       "        -5.21942878e-06,   5.21229112e-06,   5.21942778e-06,\n",
       "        -5.21942778e-06,   5.21229106e-06,   4.22455696e-02,\n",
       "         5.21942778e-06,  -5.21942778e-06,   5.21942778e-06,\n",
       "        -5.21943210e-06,  -5.21942878e-06,   4.22455696e-02,\n",
       "        -5.21936489e-06,   5.21942778e-06,  -5.21942778e-06,\n",
       "         5.21229121e-06,  -5.21942778e-06,   5.21942780e-06,\n",
       "        -5.21942878e-06,   4.22455696e-02,  -5.21942778e-06,\n",
       "        -5.21942878e-06,   5.21942778e-06,   4.22455696e-02,\n",
       "         5.21942778e-06,   5.37532905e-06,   5.21942778e-06,\n",
       "         5.37533445e-06,  -5.21942778e-06,  -5.21942778e-06,\n",
       "         5.21942778e-06,   5.37532905e-06,   5.37532905e-06,\n",
       "         5.21942778e-06,   5.21942780e-06,   5.21942780e-06,\n",
       "         4.22455696e-02,   5.21942778e-06,   5.37532905e-06,\n",
       "        -5.21936392e-06,   5.21942778e-06,   5.21942778e-06,\n",
       "         4.22473088e-02,  -3.91020096e+00])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_lik_normal(weights, x_i, r, sigma2):\n",
    "    # we calculat the log likeligood given \n",
    "    return norm.logpdf(r, np.dot(weights[:-1], np.transpose(x_i)), sigma2 * np.identity(len(x_i)))\n",
    "\n",
    "def normal_posterior(weights, x_i, br1, br2):\n",
    "    # Same parameters as log_lik.\n",
    "    # Note that we add up all the results to be able to batch process the final posterior probability\n",
    "    return np.sum(log_lik_normal(weights, x_i, r, sigma2) + log_prior(weights[:-1]))\n",
    "\n",
    "posteriorGradient = grad(normal_posterior, argnum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
